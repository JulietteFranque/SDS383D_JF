\documentclass[12pt]{amsart}

\usepackage[T1]{fontenc}
\usepackage{newpxtext}
\usepackage{newpxmath}


\addtolength{\hoffset}{-2.25cm}
\addtolength{\textwidth}{4.5cm}
\addtolength{\voffset}{-2.5cm}
\addtolength{\textheight}{5cm}
\setlength{\parskip}{0pt}
\setlength{\parindent}{15pt}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[colorlinks = true, linkcolor = black, citecolor = black, final]{hyperref}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{ marvosym }
\newcommand{\ds}{\displaystyle}
\pagestyle{myheadings}
\setlength{\parindent}{0in}

\pagestyle{empty}

\begin{document}

\thispagestyle{empty}

{\scshape SDS383D} \hfill {\scshape \Large Exercises \#1} \hfill {\scshape Juliette F}
 \medskip
\hrule
\bigskip
\bigskip

{\bf \large Bayesian inference in simple conjugate families} 
\bigskip



{\bf Part A} \\
\bigskip

We know:\\
$$P(w) = \frac{\Gamma(a+b)}{\Gamma(a).\Gamma(b)}w^{1-1}(1-w)^{b-1}$$ \\

Let $y$ be the number of successes for $n$ trials; then: \\

$$ P(y|w) = {n \choose y}w^{y}(1-w)^{n-y} $$


Then,

$$ P(w|y) \propto P(y|w)P(w) $$ \\

$$ P(w|y) \propto {n \choose y}w^{y}(1-w)^{n-y}
\frac{\Gamma(a+b)}{\Gamma(a).\Gamma(b)}w^{a-1}(1-w)^{b-1}$$ \\


$$ P(w|y) \propto {n \choose y} \frac{\Gamma(a+b)}{\Gamma(a).\Gamma(b)}w^{y+a-b}(1-w)^{n-y+b-1} 
$$ \\
 
$ w^{y+a-b}(1-w)^{n-y+b-1}  $is the beta distribution $Beta(a+y, b+n-y)$. The integral of its PDF is 1. Since the integral of the PDF of $P(w|y)$ must also be 1,  \\

$$ P(w|y) = Beta(a+y, b+n-y)$$\\


{\bf Part B} \\
\bigskip

We have $x_1 \sim Ga(a_1, 1)$ and  $x_2 \sim Ga(a_2, 1)$ . We define $y_1 = x_1/(x_1 + x_2)$ and $y_2 = x_1 + x_2$. We can write:

$$x_1 = y_1y_2  $$

and

$$x_2 = y_2 - y_1y_2  $$

Find the Jacobian\\

$$|J|= \det\begin{pmatrix} \frac{\partial x_1}{\partial y_1} & \frac{\partial x_1}{\partial y_2} \\ \frac{\partial x_2}{\partial y_1} & \frac{\partial x_2}{\partial y_2} \end{pmatrix} =det\begin{pmatrix}
  y_2 & y_1\\ 
  -y_2 & 1-y_1 
\end{pmatrix} = y_2(1-y_1) + y_1y_2 = y_2
 $$\\
 
 The joint PDF for $x_1$ and $x_2$ is \\
 
 $$\frac{1}{\Gamma(a_1)\Gamma(a_2)} x_1^{a_1-1} x_2^{a_2-1} \exp(-x_1-x_2)    $$\\
 
  The joint PDF for $y_1$ and $y_2$ is \\
  
  $$f_{y_1, y_2} = f_{x_1, x_2}(y_1, y_2) |J| $$\\
  
  $$  = \frac{1}{\Gamma(a_1)\Gamma(a_2)} (y_1 y_2)^{a_1-1} (y_2-y_1 y_2)^{a_2-1} \exp[-(y_1 y_2)-(y_2-y_1 y_2)]y_2 $$\\
  
  $$=   \frac{1}{\Gamma(a_1)\Gamma(a_2)} y_1^{a_1-1}(1-y_1)^{a_2-1}y_2^{a_2}\exp(-y_2)2 $$\\


$$ =   \left[\frac{\Gamma(a_1 + a_2)}{\Gamma(a_1)\Gamma(a_2)}y_1^{a_1-1}(1-y_1)^{a_2-1} \right] \left[ \frac{1}{\Gamma(a_1 + a_2)}y_2^{a_1+a_2-1}\exp(-y_2)  \right] $$


so 

$$ y_1 \sim Beta(a_1, a_2)$$
$$ y_2 \sim Gamma(a_1+a_2, 1)$$\\

We can use two gamma random variables and apply the transformation to obtain a beta variable.\\ 

{\bf Part C} \\
\bigskip

We know:

$$P(x_1, ...,x_n|\theta, \sigma^2) = \left(\frac{1}{\sigma \sqrt{2\pi }}\right)^n \prod_{i=1}^{n} \exp\left[-\frac{1}{2}\left(\frac{x_i-\theta}{\sigma}\right)^2\right]$$\\

Since the observations are independent. We can re-write:

$$ \prod_{i=1}^{n} \exp\left[-\frac{1}{2}\left(\frac{x_i-\theta}{\sigma}\right)^2\right] =  \exp\left[-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(x_i-\theta)^2\right]     $$\\

and 

$$\sum_{i=1}^{n}(x_i-\theta)^2 =  \sum_{i=1}^{n} (x_i^2 - 2x_i\theta + \theta^2 ) = n\bar{x^2} - 2n\bar{x}\theta +n\theta^2 $$

so



$$P(x_1, ...,x_n|\theta, \sigma^2) = \left(\frac{1}{\sigma \sqrt{2\pi }}\right)^n \exp\left(-\frac{1}{2\sigma^2}n\bar{x^2} - 2n\bar{x}\theta +n\theta^2\right)$$\\


Then, \\
$$ P(\theta) = \frac{1}{v \sqrt{2\pi }}  \exp\left[-\frac{1}{2}\left(\frac{\theta-m}{v}\right)^2\right]$$\\

Then,

$$P(\theta|x_1, ...,x_n) \propto P(x_1, ...,x_n|\theta, \sigma^2) P(\theta)$$\\

$$P(\theta|x_1, ...,x_n) \propto \left(\frac{1}{\sigma \sqrt{2\pi }}\right)^n \exp\left(-\frac{1}{2\sigma^2}n\bar{x^2} - 2n\bar{x}\theta +n\theta^2\right)\frac{1}{v \sqrt{2\pi }}  \exp\left[-\frac{1}{2}\left(\frac{x-m}{v}\right)^2\right]$$\\


We can drop the terms outside the exponential and re-arrange:

$$P(\theta|x_1, ...,x_n) \propto  \exp\left[-\frac{1}{2}\left(\frac{n\bar{x^2}+2n\bar{x}\theta+n\theta^2}{\sigma^2} + \frac{\theta^2-2m\theta+m^2}{v}\right)\right] $$\\

We can drop the terms not involving $\theta$:

$$P(\theta|x_1, ...,x_n) \propto  \exp\left[-\frac{1}{2}\left(\frac{2n\bar{x}\theta+n\theta^2}{\sigma^2} + \frac{\theta^2-2m\theta}{v}\right)\right] $$\\

Re-arranging:
$$P(\theta|x_1, ...,x_n) \propto  \exp\left[-\frac{1}{2}\left(\frac{2n\bar{x}\theta v+n\theta^2v+\theta^2\sigma^2-2m\theta\sigma^2}{\sigma^2v} \right)\right] $$\\

Completing the squares:


$$P(\theta|x_1, ...,x_n) \propto  \exp\left[-\frac{1}{2}\left(\frac{\theta^2 - 2\theta\frac{n\bar{x}v+m\sigma^2}{nv+\sigma^2}}{\frac{\sigma^2v}{nv+\sigma^2}} \right)\right] $$\\

$$P(\theta|x_1, ...,x_n) \propto  \exp\left[-\frac{1}{2}\frac{\left(\theta - \frac{n\bar{x}v+m\sigma^2}{nv+\sigma^2}\right)^2 - \left(\frac{n\bar{x}v+m\sigma^2}{nv+\sigma^2}\right)^2 }{\frac{\sigma^2v}{nv+\sigma^2}} \right] $$\\

Drop terms,

$$P(\theta|x_1, ...,x_n) \propto  \exp\left[-\frac{1}{2}\frac{\left(\theta - \frac{n\bar{x}v+m\sigma^2}{nv+\sigma^2}\right)^2 }{\frac{\sigma^2v}{nv+\sigma^2}} \right] $$\\

This has the form of a normal distribution, therefore:
$$P(\theta|x_1, ...,x_n) \sim N  \left(\frac{n\bar{x}v+m\sigma^2}{nv+\sigma^2},  \frac{\sigma^2v}{nv+\sigma^2}  \right)$$\\

{\bf Part D} \\
\bigskip

We know

$$P(w) \sim Ga(a,b)$$\\

and 

$$P(x_1, ..., x_n|w) =  \left(\frac{w}{2\pi}\right)^{\frac{n}{2}}\prod_{i=1}^{n}\exp \left[ -\frac{w}{2}(x_i-\theta)^2 \right] $$\\

or 

$$P(x_1, ..., x_n|w) =  \left(\frac{w}{2\pi}\right)^{\frac{n}{2}}\exp \left[ -\frac{w}{2} \sum_{i=1}^{n}(x_i-\theta)^2 \right] $$\\

Then,
$$P(w|x_1, ..., x_n) \propto  \left(\frac{w}{2\pi}\right)^{\frac{n}{2}}\exp \left[ -\frac{w}{2} \sum_{i=1}^{n}(x_i-\theta)^2 \right] \frac{b^a}{\Gamma(a)}w^{a-1}\exp(-bw) $$\\

or,

$$P(w|x_1, ..., x_n) \propto  w^{a-1+\frac{n}{2}}\exp \left[ -w \left( \sum_{i=1}^{n}\frac{(x_i-\theta)^2}{2} +b \right)\right]  $$\\

or


$$P(w|x_1, ..., x_n) \sim Ga\left(a+\frac{n}{2}, \sum_{i=1}^{n}\frac{(x_i-\theta)^2}{2} +b  \right)$$

$\sigma^2$ will be an IG distribution with the same parameters.\\


{\bf Part E} \\
\bigskip

We know

$$ x_i \sim N(\theta, {\sigma_i}^2)$$


and 

$$ P(\theta) \sim N(m,v)$$

so

$$ P(x_1, ..., x_n | \theta, \sigma_1^2, ..., \sigma_n^2) = \prod_{i=1}^{n} \frac{1}{\sigma_i \sqrt{2\pi}}\exp\left[-\frac{1}{2}\left(\frac{x_i-\theta}{\sigma_i}\right)^2\right] $$\\
or
$$ P(x_1, ..., x_n | \theta, \sigma_1^2, ..., \sigma_n^2) \propto  \exp\left[-\frac{1}{2}\sum_{i=1}^{n}\left(\frac{x_i-\theta}{\sigma_i}\right)^2\right] $$\\

Then,\\

$$P( \theta|x_1, ..., x_n ) \propto P( x_1, ..., x_n|\theta)P(\theta)$$
 or\\
 
 $$P( \theta|x_1, ..., x_n ) \propto \exp\left[-\frac{1}{2} \left(\sum_{i=1}^{n}\left(\frac{x_i-\theta}{\sigma_i}\right)^2 + \frac{(\theta-m)^2}{v}\right)\right] $$\\
 
 or \\
 
  $$P( \theta|x_1, ..., x_n ) \propto \exp\left[-\frac{1}{2} \left(\sum_{i=1}^{n}\frac{x_i^2 - 2x_i\theta+\theta^2}{\sigma_i^2} + \frac{\theta^2-2m\theta + m^2}{v}\right)\right] $$\\
  
  Dropping terms and re-arranging\\
  
    $$P( \theta|x_1, ..., x_n ) \propto \exp\left[-\frac{1}{2} \left( -2\sum_{i=1}^{n}\frac{x_i^2}{\sigma_i^2} + \theta^2\sum_{i=1}^{n}\frac{1}{\sigma_i^2} +\frac{\theta^2}{v} -\frac{2m\theta}{v}\right) \right] $$\\
    
    
    or\\
    
        $$P( \theta|x_1, ..., x_n ) \propto \exp\left[-\frac{1}{2}\left( \sum_{i=1}^{n}\frac{1}{\sigma_i^2} + \frac{1}{v} \right) \left(
        \theta^2 -2\theta \frac{\frac{x_i^2}{\sigma_i^2}+\frac{m}{v} }{\sum_{i=1}^{n}\frac{1}{\sigma_i^2} + \frac{1}{v}}   \right)\right]$$
        
        
or 

  $$P( \theta|x_1, ..., x_n ) \propto \exp\left[-\frac{1}{2}\left( \sum_{i=1}^{n}\frac{1}{\sigma_i^2} + \frac{1}{v} \right) \left(
        \theta - \frac{\frac{x_i^2}{\sigma_i^2}+\frac{m}{v} }{\sum_{i=1}^{n}\frac{1}{\sigma_i^2} + \frac{1}{v}}   \right)^2\right]$$
        
so we have


$$P( \theta|x_1, ..., x_n ) \sim   N \left(\frac{\frac{x_i^2}{\sigma_i^2}+\frac{m}{v} }{\sum_{i=1}^{n}\frac{1}{\sigma_i^2} + \frac{1}{v}}, \left( \sum_{i=1}^{n}\frac{1}{\sigma_i^2} + \frac{1}{v} \right)^{-1}  \right)$$\\

{\bf Part F} \\
\bigskip
    
    
We know

$$ (x|\omega) \sim N(m, \omega^{-1})$$\\

Where $$\omega = \frac{1}{\sigma^2}$$\\

And

$$\omega \sim Ga\left(  \frac{a}{2} , \frac{b}{2}         \right)$$\\

$$P(x) = \int_{0}^{\infty}P(x|\omega)P(\omega)d\omega$$\\

$$P(x) =\int_{0}^{\infty} \left(\frac{\omega}{2\pi}\right)^\frac{1}{2}\exp \left[ -\frac{1}{2}\left((x-m)^2\omega\right)\right] \frac{\frac{b}{2}^\frac{a}{2}}{\Gamma{\frac{a}{2}}}\omega^{\frac{a}{2}-1}  \exp\left (-\frac{b}{2}\omega \right)  d\omega $$\\

Drop terms\\

$$P(x) \propto\int_{0}^{\infty} \exp \left[ -\frac{1}{2}\left((x-m)^2\omega\right)\right] \omega^{\frac{a}{2}-\frac{1}{2}}  \exp\left (-\frac{b}{2}\omega \right)  d\omega $$\\

Re-arrange\\

$$P(x) \propto\int_{0}^{\infty} \exp \left[ -\omega\left(\frac{(x-m)^2+b}{2}\right)\right] \omega^{\frac{a}{2}-1}    d\omega $$\\


This looks like a gamma function:\\

$$ \frac{\Gamma(z)}{\beta^z} = \int_{0}^{\infty}x^{z-1}e^{-\beta x}dx$$\\

where $$ z \equiv \frac{1}{2}(a+1)$$\\

and \\

$$\beta \equiv \frac{(x-m)^2+b}{2}$$\\

so 

$$P(x) \propto \Gamma\left[\frac{1}{2}(a+1)\right] \left[\frac{(x-m)^2+b}{2}\right]^{-\frac{1}{2}(a+1)}$$\\

Drop constants 

$$P(x) \propto \left[(x-m)^2+b\right]^{-\frac{1}{2}(a+1)}$$\\

or 
$$P(x) \propto \left[\frac{(x-m)^2}{b}+1\right]^{-\frac{1}{2}(a+1)}$$\\


or

$$P(x) \propto \left[\frac{1}{a}\frac{(x-m)^2}{\frac{b}{a}}+1\right]^{-\frac{1}{2}(a+1)}$$\\

This is a t distribution with $loc=m$,  $scale=\sqrt{b/a
}$, and $df=a$.

\newpage
{\large \bf The multivariate normal distribution}
\bigskip
\bigskip

{\bf Part A}\\
\bigskip

Prove that 
$$cov(x)=E(xx^T)-\mu\mu^T$$\\

Start with \\

$$cov(x)=E\{(x-\mu)(x-\mu)^T\}$$\\


$$cov(x)=E\{(x-\mu)(x^T-\mu^T)\}$$\\

$$cov(x)=E\{xx^T-x\mu^T-\mu x^T+\mu\mu^T\}$$\\

$$cov(x)=E\{xx^T\} -E\{x\}\mu^T -\mu E\{x^T\}+\mu\mu^T\}$$\\



$$cov(x)=E\{xx^T\}-2\mu \mu^T+\mu\mu^T $$\\

$$cov(x)=E\{xx^T\}-\mu\mu^T$$\\


Prove that\\

$$cov(Ax+b) = Acov(x)A^T $$

Start with

$$cov(Ax+b) = E\{[Ax+b-E(Ax+b)][Ax+b-E(Ax+b)]^T\}$$\\


$$cov(Ax+b) = E\{[Ax+b-AE(x)-b][Ax+b-AE(x)-b]^T\}$$\\

$$cov(Ax+b) = E\{[Ax-AE(x)][Ax-AE(x)]^T\}$$\\

$$cov(Ax+b) = E\{A[x-E(x)] [x^TA^T-E(x)^TA^T]\}$$\\

$$cov(Ax+b) = E\{A[x-E(x)] [x^T-E(x)^T]A^T\}$$\\

$$cov(Ax+b) = AE\{[x-\mu] [x^T-\mu^T]\}A^T$$\\

$$cov(Ax+b) = Acov(x)A^T$$\\

{\bf Part B}\\
\bigskip

Derive the PDF of $z = (z_1, ..., z_p)$

$$p(z) = \prod_{i=1}^{p}\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}z_i^2\right) $$\\

$$=\left(\frac{1}{\sqrt{2\pi}} \right)^{p/2} \exp\left(-\frac{1}{2}\sum_{i=1}^{p} z_i^2\right) $$\\

$$ =\left(\frac{1}{\sqrt{2\pi}} \right)^{p/2} \exp\left(-\frac{1}{2} z^Tz\right) $$




Derive the MGF of $z$

$$M_{zi}(t_i) = E[\exp(tz)] =\int_{-\infty}^{\infty} \exp(tz) \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2}z^2\right)dz $$\\


$$=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty} \exp\left(-\frac{1}{2}z^2 +t_iz\right)dz $$\\

$$=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty} \exp\left[-\frac{1}{2}(z^2 -2t_i)\right]dz $$\\

$$=\frac{1}{\sqrt{2\pi}}\exp\left(\frac{1}{2}t^2\right)\int_{-\infty}^{\infty} \exp\left[-\frac{1}{2}(z-t_i)^2\right]dz $$\\

$$=\exp\left(\frac{1}{2}t_i^2\right)$$


Then,\\

$$M_{z}(t) = \prod_{i=1}^{p} M_{zi}(t_i) =\prod_{i=1}^{p} \exp\left(\frac{1}{2}t_i^2\right) =\exp\left(\frac{1}{2}\sum_{i=1}^{n}t_i^2\right) =  \exp\left(\frac{1}{2}t^Tt\right)$$\\


{\bf Part C}\\
\bigskip

For a univariate normal distribution:


$$ M_z = E\left\{\exp\left(  mt + \frac{1}{2}vt^2   \right) \right\} $$

Then,

$$ m = E(z) = E(a^Tx) = a^T\mu $$

$$ v= var(z) = var(a^Tx) = a^Tvar(x)a = a^T \Sigma a $$\\

Plug $m$ and $v$ into the $M_z$ equation


$$ M_z =E\{\exp(ta^Tx )\} =\exp\left(  a^T\mu t + \frac{1}{2}a^T \Sigma a t^2   \right)  $$

Let $t=1$

$$ M_z =E\{\exp(a^Tx )\}  = \exp\left(  a^T\mu  + \frac{1}{2}a^T \Sigma a \right)  $$\\

If we replace $a$ with $t$, this has the same form as in the question:\\

$$ M_z =\exp(t^Tx )\}  = \exp\left(  t^T\mu  + \frac{1}{2}t^T \Sigma t \right)  $$


{\bf Part D}\\
\bigskip

Let $x=Lz+\mu$\\

The MGF for $z$ is\\

$$ M_x = E\{\exp(t^Tz )\} $$\\

$$= E\{\exp(t^T(Lz+\mu) )\} $$\\


$$= E\{\exp(t^TLz \exp\{ t^T\mu )\} $$\\

$$= \exp\{ t^T\mu ) E\{\exp(t^TLz\} $$\\


$$= \exp( t^T\mu ) E\{\exp[(L^Tt)^Tz]\} $$\\


$$= \exp\left\{ t^T\mu +\frac{(L^Tt)^T(L^Tt)}{2}\right\} $$\\

$$= \exp\left\{ t^T\mu +\frac{t^TLL^Tt}{2}\right\} $$\\



This has the form of a multivariate normal MGF with mean $\mu$ and variance $LL^T$.\\


{\bf Part E}\\
\bigskip

We know the MFG for $z$, and $x=Lz+\mu$

$$M_z(t) = E\{\exp(t^Tz)\}= E\{\exp[t^TL^{-1}(x-\mu)]\}$$\\

$$ = E\{\exp(t^Tz)\}= \exp(-t^TL^{-1}\mu)E\{\exp[t^TL^{-1}x]\}$$\\

$$ = E\{\exp(t^Tz)\}= \exp(-t^TL^{-1}\mu)E\{\exp[(L^{-T}t)^Tx]\}$$\\

$$ = E\{\exp(t^Tz)\}= \exp(-t^TL^{-1}\mu)\exp\left[ \left(L^{-T}t\right)^T\mu  + \frac{1}{2}\left(L^{-T}t\right)^T \Sigma \left(L^{-T}t\right) \right]$$\\


We can write $\Sigma$ as $LL^T$ because it is symmetric and square.

$$ = E\{\exp(t^Tz)\}= \exp\left(-t^TL^{-1}\mu\right)\exp\left[  (L^{-T}t)^T\mu  + \frac{1}{2}\left(L^{-T}t\right)^T LL^T\left(L^{-T}t\right) \right]$$\\

$$= E\{\exp(t^Tz)\}= \exp\left(-t^TL^{-1}\mu\right)\exp\left[  (L^{-T}t)^T\mu  + \frac{1}{2}\left(t^TL^{-1} LL^TL^{-T}t\right) \right]$$\\


$$ = E\{\exp(t^Tz)\}= \exp\left(-t^TL^{-1}\mu\right)\exp\left[  (L^{-T}t)^T\mu  + \frac{1}{2}\left(t^Tt\right) \right]$$\\

$$ =\exp\left(\frac{t^Tt}{2}\right)$$\\

Which is the MGF for a standard multivariate normal distribution.\\

{\bf Algorithm to simulate a multivariate normal distribution with known covariance matrix and means:}

\begin{itemize}
    \item Make n standard univariate distributions
    \item Generate vector of length n containing means
    \item Generate convariance matrix of size $n \times n$
    \item Get desired distribution using $x = Lz + \mu$
\end{itemize}
\bigskip
\bigskip
{\bf Part F}\\
\bigskip

The PDF of a standard multivariate normal is:



$$ f(z)=\left(\frac{1}{\sqrt{2\pi}} \right)^{p/2} \exp\left(-\frac{1}{2} z^Tz\right) $$

Then,

$$ x=g(z) = Lz+\mu$$

or

$$ z= g^{-1}(x) = L^{-1}(x - \mu)$$

and

$$f_x = f_z(g^{-1}(x)).|J| $$

Where \\
$$|J| = det(L^{-1}) = \frac{1}{det(L)} = \frac{1}{det(\Sigma L^{-T}) } = \frac{det(L)}{det(\Sigma)}=\frac{1}{det(\Sigma)|J|}$$\\

so 

$$|J| = det(\Sigma)^{-\frac{1}{2}} = det(\Sigma^{-\frac{1}{2}})  $$

 $$f_x = \left(\frac{1}{\sqrt{2\pi}} \right)^{p/2} det(\Sigma^{-\frac{1}{2}}) \exp\left(-\frac{1}{2} [L^{-1}(x - \mu)]^TL^{-1}(x - \mu)\right) $$

 $$= \left(\frac{1}{\sqrt{2\pi}} \right)^{p/2} det(\Sigma^{-\frac{1}{2}}) \exp\left(-\frac{1}{2} (x - \mu)^TL^{-T}L^{-1}(x - \mu)\right) $$

 $$= \left(\frac{1}{\sqrt{2\pi}} \right)^{p/2} det(\Sigma^{-\frac{1}{2}}) \exp\left(-\frac{1}{2} (x - \mu)^T(L^TL)^{-1}(x - \mu)\right) $$
 
  $$= \left(\frac{1}{\sqrt{2\pi}} \right)^{p/2} det(\Sigma^{-\frac{1}{2}}) \exp\left(-\frac{1}{2} (x - \mu)^T\Sigma^{-1}(x -\mu)\right) $$


{\bf Part G}\\
\bigskip

We have $ x_1\sim N(\mu_1, \Sigma_1) $ and $ x_2\sim N(\mu_2, \Sigma_2) $ with $y=Ax_1 + Bx_2$\\

Calculate the MGF of $y$:

$$MGF_y(t) = E\{\exp(t^Ty)    \}     $$\\

$$= E\{\exp(t^T(Ax_1 + Bx_2))    \}     $$\\


$$= E\{\exp(t^TAx_1) +  \exp(t^T Bx_2))    \}     $$\\


$$= E\{\exp(t^TAx_1) +  \exp(t^T Bx_2))    \}     $$\\


$$= E\{\exp( [A^Tt]^T x_1   ) +  \exp([B^Tt]^T x_2 )    \}     $$\\

$$ = \exp\left(  [A^Tt]^T\mu_1  + \frac{1}{2}[A^Tt]^T \Sigma_1 [A^Tt]  +  [B^Tt]^T\mu_2  + \frac{1}{2}[B^Tt]^T \Sigma_2 [B^Tt]\right)  $$


$$ = \exp\left(  t^TA \mu_1  + \frac{1}{2}t^TA \Sigma_1 [A^Tt]  +  t^TB\mu_2  + \frac{1}{2}t^TB \Sigma_2 [B^Tt]\right)  $$\\


$$ = \exp\left( t^T[A\mu_1 + B\mu_2] + \frac{1}{2}t^T[A\Sigma_1A^T+B\Sigma_2 B^T]t          \right)  $$\\


This has the form of a multivariate normal mgf, so its PDF is as shown in Part F with     $\mu \equiv A\mu_1+B\mu_2$ and $\Sigma \equiv A\Sigma_1A^T+B\Sigma_2 B^T$\\
\bigskip

{\bf \large Conditionals and Marginals} 
\bigskip

\bigskip

{\bf Part A}\\
\bigskip



From the previous question, for $y = Ax_1 + Bx_2$

 $$ MGF_y= \exp\left( t^T[A\mu_1 + B\mu_2] + \frac{1}{2}t^T[A\Sigma_1A^T+B\Sigma_2 B^T]t          \right)  $$\\

Here let $x_1 = Ax$, where $A = [I_{k \times k} | 0_{k\times(p-k)}]$, so $x_1$ has is multivariate normal with mean  $ A \mu$ and covariance $A\Sigma A^T$

We have:

$$ E(x_1) = A\mu = \begin{bmatrix}
I & 0 
\end{bmatrix}   \begin{bmatrix}
\mu_1 \\
\mu_2 
\end{bmatrix}  = \mu_1$$

and

$$ cov(x_1) = A\Sigma A^T = \begin{bmatrix}
I & 0 
\end{bmatrix} \begin{bmatrix}
\Sigma_{11} & \Sigma_{12} \\
\Sigma_{21} & \Sigma_{22} 
\end{bmatrix}   \begin{bmatrix}
I \\
0
\end{bmatrix} = \begin{bmatrix}
I & 0 
\end{bmatrix}  \begin{bmatrix}
\Sigma_{11} \\
\Sigma_{21} 
\end{bmatrix} = \Sigma_{11}
$$\\

The marginal distribution of $x_1$ is a multivariate normal with these parameters.\\


{\bf Part B}\\
\bigskip

Using identity:


$$ \Omega_{11} = (\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21})^{-1} $$

$$ \Omega_{12} = -\Sigma_{11}^{-1}\Sigma_{12}(\Sigma_{22}-\Sigma_{21}\Sigma_{11}^{-1} \Sigma_{12})^{-1}              $$

$$ \Omega_{21} = -\Sigma_{22}^{-1}\Sigma_{21} (\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21})^{-1}       $$

$$ \Omega_{22} = (\Sigma_{22}-\Sigma_{21}\Sigma_{11}^{-1} \Sigma_{12})^{-1}              $$\\


{\bf Part C}\\
\bigskip
$$f(x_1|x_2) = \frac{f(x_1, x_2)}{f(x_2)}$$ 

We know 


$$f(x_1,x_2) \propto   \exp\left(-\frac{1}{2} (x - \mu)^T\Omega(x -\mu)\right)  $$\\

so
 $$f(x_1|x_2) \propto  \exp\left(-\frac{1}{2} (x - \mu)^T\Omega(x -\mu)\right) $$\\
 
 $$f(x_1,x_2) \propto   \exp\left(-\frac{1}{2}\begin{bmatrix}
x_1 - \mu_1 \\
x_2 - \mu_2 
\end{bmatrix}^T \begin{bmatrix}
\Omega_{11} & \Omega_{12} \\
\Omega_{21} & \Omega_{22} 
\end{bmatrix}  \begin{bmatrix}
x_1 - \mu_1 \\
x_2 - \mu_2 
\end{bmatrix}\right)  $$\\
  
$$\propto \exp\left\{ -\frac{1}{2}\begin{bmatrix} (x_1-\mu_1)^T\Omega_{11} + (x_2-\mu_2)^T\Omega_{21}& (x_1-\mu_1)^T\Omega_{12} + (x_2-\mu_2)^T\Omega_{22}\end{bmatrix} \begin{bmatrix}
x_1 - \mu_1 \\
x_2 - \mu_2 
\end{bmatrix} \right \}$$\\

\begin{equation*}
\begin{split}
\propto \exp \{ (-1/2)[(x_1-\mu_1)^T\Omega_{11}(x_1 - \mu_1) +(x_2-\mu_2)^T\Omega_{21}(x_1 - \mu_1) + \\ (x_1-\mu_1)^T\Omega_{12}(x_2 - \mu_2 ) +(x_2-\mu_2)^T\Omega_{22}(x_2 - \mu_2 )]\}
\end{split}
\end{equation*}\\

using $\Omega_{12}^T = \Omega_{21}$ and dropping last term\\


\begin{equation*}
\propto \exp \{ (-1/2)[(x_1-\mu_1)^T\Omega_{11}(x_1 - \mu_1)  + 2 (x_1-\mu_1)^T\Omega_{12}(x_2 - \mu_2 ) ]\}
\end{equation*}\\


Complete the square\\

This looks like (where A is square symmetric) \\

$$(x+h)^TA(x+h) = x^TAx+2x^TAh+h^TAh $$\\

Where $A \equiv \Omega_{11}$, $x \equiv (x_1 - \mu_1)$, and $h \equiv \Omega_{11}^{-1}\Omega_{12}(x_2-\mu_2)$ \\


so, ignoring last term\\

\begin{equation*}
\propto \exp \{ (-1/2) [ (x_1 - \mu_1)+\Omega_{11}^{-1}\Omega_{12}(x_2-\mu_2)]^T\Omega_{11} [ (x_1 - \mu_1)+\Omega_{11}^{-1}\Omega_{12}(x_2-\mu_2)]\}
\end{equation*}\\


\begin{equation*}
\propto \exp \{ (-1/2) [ x_1 - (\mu_1+\Omega_{11}^{-1}\Omega_{12}(x_2-\mu_2))]^T\Omega_{11} [ x_1 - (\mu_1+\Omega_{11}^{-1}\Omega_{12}(x_2-\mu_2))]\}
\end{equation*}\\

Where \\

$$ \Omega_{11} = (\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21})^{-1} $$

and \\

$$ \Omega_{11}^{-1} \Omega_{12} = \Omega_{11}^{-1}\Omega_{21}^T =\Omega_{11}^{-1} [\Sigma_{22}^{-1}\Sigma_{21}\Omega_{11}]^T = \Omega_{11}^{-1}\Omega_{11}\Sigma_{21}^T\Sigma_{22}^{-T} = \Sigma_{21}\Sigma_{22}^{-1}$$\\

This has the form of a multivariate normal:

$$ \sim N(\mu_1+\Sigma_{21}\Sigma_{22}^{-1}[x_2-\mu_2],\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21})$$\\

{\bf \large Multiple regression: three classical principles for inference} 
\bigskip

{\bf Part A}\\
\bigskip
\bigskip
\begin{itemize}
    \item Least squares 
\end{itemize}
\medskip

Find $\beta$ such that $(y-X\beta)^T (y-X\beta)$ is minimized.
$$(y-X\hat{\beta})^T (y-X\hat{\beta}) $$
$$=y^Ty-y^TX\hat{\beta} -\hat{\beta}^TX^Ty  + \hat{\beta}^TX^TX\hat{\beta}$$
$$=y^Ty-(X\hat{\beta})^Ty -(X\hat{\beta})^Ty  + \hat{\beta}^TX^TX\hat{\beta}$$
$$=y^Ty -2(X\hat{\beta})^Ty  + \hat{\beta}^TX^TX\hat{\beta}$$

Let

$$ \frac{\partial}{\partial \hat{\beta}}[y^Ty -2\hat{\beta}^TX^Ty  + \hat{\beta}^TX^TX\hat{\beta}] = 0$$
$$ -2y^TX + 2X^TX\hat{\beta} = 0$$
$$ 2X^TX\hat{\beta} = 2y^TX $$
$$ \hat{\beta} = (X^TX)^{-1}y^TX $$\\

\begin{itemize}
    \item Maximum Likelihood
\end{itemize}
\medskip

We assume $\epsilon_i \sim N(0, \sigma^2)$
$$ P(y_i|\hat{\beta}, \sigma^2) \propto \prod_{i=1}^{n} \exp\left\{-\frac{1}{2\sigma^2}(y_i - x_i^T\hat{\beta})^2 \right \}$$

$$  \propto \exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i - x_i^T\hat{\beta})^2 \right \}$$


$$  \propto \exp\left\{-\frac{1}{2\sigma^2}(y-X\hat{\beta})^T(y-X\hat{\beta}) \right \}$$

We can take the log and maximize


$$ \frac{\partial}{\partial \hat{\beta}}[y^Ty -2\hat{\beta}^TX^Ty  + \hat{\beta}^TX^TX\hat{\beta}] = 0$$

We end up with the same thing as before. \\

\begin{itemize}
    \item Moment matching
\end{itemize}
\medskip

Find $\hat{\beta}$ so that $cov(\epsilon, x_j)=0$

$$ \frac{1}{n-1}\sum_{i=1}^{n}(x_{ij}-\bar{x_j})\epsilon_i=0$$

$$X^T \epsilon = 0$$ 

Assuming $\bar{x}=0$.\\


Then,

$$ X^T\epsilon =0$$
$$ X^T(y-X\hat{\beta}) =0$$
$$ X^Ty-X^TX\hat{\beta} =0$$
$$ \hat{\beta}  =(X^TX)^{-1} X^Ty$$

Note $$ y \sim N(X\beta, \sigma^2I)$$\\

$$ E(\hat{\beta}) = (X^TX)^{-1} X^TE(y) =(X^TX)^{-1} X^T X\beta = \beta$$



$$ cov(\hat{\beta}) = (X^TX)^{-1} X^Tcov(y)[(X^TX)^{-1}X^T]^{T}$$
$$ = (X^TX)^{-1} X^T(\sigma^2I)[(X^TX)^{-1}X^T]^{T}$$
$$ = (X^TX)^{-1} X^T(\sigma^2I)X(X^TX)^{-T}$$
$$ = (X^TX)^{-1} X^T(\sigma^2I)X(X^TX)^{-1}$$
$$ =\sigma^2 [(X^TX)^{-1} X^TX(X^TX)^{-1}]$$
$$ =\sigma^2(X^TX)^{-1}$$


{\bf Part B}\\
\bigskip
\bigskip

We want to maximize


$$ -\frac{1}{2}(y-X\hat{\beta})^T\Sigma^{-1}(y-X\hat{\beta}) $$
$$ \propto \frac{\partial}{\partial\hat{\beta}} [-y^T\Sigma^{-1}y+\hat{\beta}^TX^T\Sigma^{-1}y+y^T \Sigma^{-1}X\hat{\beta}-\hat{\beta}^TX^T\sigma^{-1}X\hat{\beta} ] = 0 $$

$$ 2y^T\Sigma^{-1}X-2X^T\Sigma^{-1}X\hat{\beta}=0$$

$$ \hat{\beta}=(X^T\Sigma^{-1}X)^{-1}y^T\Sigma^{-1}X =(X^T\Sigma^{-1}X)^{-1}X^T\Sigma^{-1}y $$

As before, 

$$ E(\hat{\beta}) = \beta$$


$$ cov(\hat{\beta}) = [(X^T\Sigma^{-1}X)^{-1}\Sigma^{-1}]^{-1} X^Tcov(y)[(X^T\Sigma^{-1}X)^{-1}X^T\Sigma^{-1}]^{T}$$
$$  = [(X^T\Sigma^{-1}X)^{-1}X^T\Sigma^{-1}] \Sigma[(X^T\Sigma^{-1}X)^{-1}X^T\Sigma^{-1}]^{T}$$
$$ = (X^T\Sigma^{-1}X)^{-1}$$\\
{\bf Part C}\\
\bigskip
\bigskip

We want to maximize


$$ -\frac{1}{2}(y-X\hat{\beta})^T\Sigma^{-1}(y-X\hat{\beta}) $$\\
Where W is a diagonal precisions matrix with entries $w_i = \frac{1}{\sigma_i^2}$\\


We get\\

$$ \hat{\beta} =(X^TWX)^{-1}X^TWy $$\\


\end{document}