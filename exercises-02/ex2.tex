\documentclass[12pt]{amsart}

\usepackage[T1]{fontenc}
\usepackage{newpxtext}
\usepackage{newpxmath}


\usepackage{amsmath}
\addtolength{\hoffset}{-2.25cm}
\addtolength{\textwidth}{4.5cm}
\addtolength{\voffset}{-2.5cm}
\addtolength{\textheight}{5cm}
\setlength{\parskip}{0pt}
\setlength{\parindent}{15pt}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[colorlinks = true, linkcolor = black, citecolor = black, final]{hyperref}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{ marvosym }
\newcommand{\ds}{\displaystyle}
\pagestyle{myheadings}
\setlength{\parindent}{0in}

\pagestyle{empty}

\begin{document}

\thispagestyle{empty}

{\scshape SDS383D} \hfill {\scshape \Large Exercises \#2} \hfill {\scshape Juliette F}
 \medskip
\hrule
\bigskip
\bigskip

{\bf \large Exponential families} 
\bigskip



{\bf Part A} 
\bigskip

Show that the normal, binomial and poisson distribution are exponential

\begin{itemize}
    \item $Y \sim N(\mu, \sigma^2) $
\end{itemize}
\bigskip
\bigskip

\begin{align*}
f(y|\mu,\sigma^2)&= -\frac{1}{\sqrt{2\pi}\sigma} \exp \left\{ -\frac{1}{2} \frac{(y-\mu)^2}{\sigma^2}\right\}\\
&=\exp \left\{ -\frac{1}{2} \frac{y^2-2y\mu + \mu^2}{\sigma^2}  -\frac{1}{2}\log ( 2\pi\sigma^2)\right\} \\
&=  \exp \left\{ -\frac{1}{2} \frac{-2y\mu + \mu^2}{\sigma^2} + \frac{y^2}{2\sigma^2} -\frac{1}{2}\log ( 2\pi\sigma^2)\right\}
\end{align*}

\bigskip
\bigskip

so $b(\theta) = \frac{1}{2}\mu^2$, $a(\phi) = \sigma^2$, and $ c(y;\phi) =-\frac{1}{2}\log ( 2\pi\sigma^2) + \frac{y^2}{2\sigma^2} $

\bigskip
\bigskip


\begin{itemize}
    \item $Y=Z/N $ where $Z \sim Binom(N, P)$
\end{itemize}
\medskip


\begin{align*}
f(yn|n,p)&= {n \choose yn} p^{yn} (1-p)^{n-yn}\\
\log[f(yn|n,p)]&= \log[{n \choose yn} p^{yn} (1-p)^{n-yn}]\\
&=\log[{n \choose yn}]+(yn)\log[p] + n(1-y) \log(1-p)]\\
&=\log[{n \choose yn}]+(yn)\log[p/(1-p)] -(yn) \log(1-p)]\\
\end{align*}\\



We see that $\theta \equiv\log[p/(1-p)] $ and $c(y, \phi) \equiv \log[{n \choose ny}] - \log(N) $. We can re-write $p = \exp\theta/(1+\exp\theta)$ and $1-p = 1/(1+\exp\theta)$. So, $k \log(1-p) =  \log(1+\exp\theta)$, and $b(\theta) \equiv  \log(1+\exp\theta)$, and $a(\phi)=1/n $. 

\bigskip
\bigskip


\begin{itemize}
    \item $Y \sim Poisson(\lambda) $
\end{itemize}
\bigskip
\bigskip

\begin{align*}
f(k|\lambda)&= \lambda^k \frac{\exp(-\lambda)}{k!}\\
&= \exp(-\lambda +\log[\lambda^k] - \log[k!])\\
&= \exp(-\lambda +k\log[\lambda] - \log[k!])\\
\end{align*}\\

So $\theta \equiv log(\lambda)$, $b(\theta) = \lambda \equiv \exp \theta$, $a(\phi)\equiv1$, $c(k,\phi)\equiv-log[k!]$

\bigskip

\bigskip
{\bf Part B} \\
\bigskip
Prove that the mean of the score function is 0. Start with:
\begin{align*}
     \int_{-\infty}^{\infty} f(x|\theta) \,dx = 1
\end{align*}


\begin{align*}
     \int_{-\infty}^{\infty} \frac{\partial}{\partial \theta}f(y|\theta) \,dy = 0
\end{align*}

\begin{align*}
     \int_{-\infty}^{\infty} \frac{f(y|\theta) }{f(y|\theta) }\frac{\partial}{\partial \theta}f(y|\theta) \,dy = 0
\end{align*}


\begin{align*}
     \int_{-\infty}^{\infty} f(y|\theta)\frac{\partial}{\partial \theta}ln[f(y|\theta)] \,dy = 0
\end{align*}

or 


\begin{align*}
     E\{\frac{\partial}{\partial \theta}ln[f(y|\theta)] \} = 0
\end{align*}

In our case we are looking for

\begin{align*}
     E\{\frac{\partial}{\partial \theta}\sum_{i=1}^{n}  ln  [f(y_i|\theta)] \} 
\end{align*}
\begin{align*}
     =E\{ \sum_{i=1}^{n} \frac{\partial}{\partial \theta} ln [f(y_i|\theta)] \} 
\end{align*}

Since $ E(a+b) = E(a) + E(b)$

\begin{align*}
   E\{ \sum_{i=1}^{n} \frac{\partial}{\partial \theta} ln [f(y_i|\theta)] \}  = E(s|\theta)=0
\end{align*}\\

Prove that the variance of the score function is $var[s(\theta)]=-H(\theta)$

\begin{align*}
   0 &=  E[s(\theta)]\\
   0 &= \int_{-\infty}^{\infty} f(y|\theta)\frac{\partial}{\partial \theta}ln[f(y|\theta)] \,dy\\
   0 &= \int_{-\infty}^{\infty} \frac{\partial}{\partial \theta^T} \{ f(y|\theta)\frac{\partial}{\partial \theta}ln[f(y|\theta)] \ \}dy\\
   &=  \int_{-\infty}^{\infty}f(y|\theta) \frac{\partial^2}{\partial \theta^T\theta}  ln[f(y|\theta)]dy +  \int_{-\infty}^{\infty}\frac{\partial}{\partial \theta}ln[f(y|\theta)] \frac{\partial}{\partial \theta^T}  f(y|\theta)   \ dy\\
   &= E(\frac{\partial^2}{\partial \theta^T\theta}  ln[f(y|\theta)]) + \int_{-\infty}^{\infty}\frac{\partial}{\partial \theta}ln[f(y|\theta)] \frac{\partial}{\partial \theta^T}  f(y|\theta) \frac{f(y|\theta)}{f(y|\theta)} dy\\
   &= E(\frac{\partial^2}{\partial \theta^T\theta}  ln[f(y|\theta)]) + \int_{-\infty}^{\infty}\frac{\partial}{\partial \theta}ln[f(y|\theta)] \frac{\partial}{\partial \theta^T}  ln[f(y|\theta)]  f(y|\theta)dy\\
   &=E(\frac{\partial^2}{\partial \theta^T\theta}  ln[f(y|\theta)]) + E(\frac{\partial}{\partial \theta}ln[f(y|\theta)] \frac{\partial}{\partial \theta^T}  ln[f(y|\theta)])
\end{align*}\\

or, since the mean of the score function is zero:


\begin{align*}
    E[s(\theta)s(\theta)^T]= var[s(\theta)] =-E\left(\frac{\partial^2 ln[f(y|\theta)]}{\partial \theta^T\theta} \right ) 
\end{align*}


\bigskip
{\bf Part C} \\
\bigskip

If $Y \sim f(y|\theta, \phi)$ prove that $E(Y)=b'(\theta)$ and $var(Y)=a(\phi)b''(\theta)$


\begin{align*}
   \frac{\partial}{\partial \theta}ln[f(y; \beta, \phi)] &=  \frac{\partial}{\partial \theta} \left\{ \frac{y\theta-b(\theta)}{a(\phi)}+c(y;\phi) \right\}\\
   &= \frac{y-b'(\theta)}{a(\phi)}\\
\end{align*}

Since we know the expected value of this is zero from previous part:

\begin{align*}
   E\left(\frac{y-b'(\theta)}{a(\phi)}\right)=0
\end{align*}
\begin{align*}
   E(y) = b'(\theta)
\end{align*}

For the variance, 
\begin{align*}
    var[s(\theta)] &=-E\left(\frac{\partial^2 ln[f(y|\theta)]}{\partial \theta^T\theta} \right ) \\
    &=-E\left(\frac{\partial^2}{\partial \theta^T\theta}\left\{ \frac{y\theta-b(\theta)}{a(\phi)}+c(y;\phi) \right\} \right )\\
    &=-E\left( -\frac{b''(\theta)}{a(\phi)}\right )\\
    &=  \frac{b''(\theta)}{a(\phi)}
\end{align*}


or 
\begin{align*}
   var( \frac{\partial}{\partial \theta }ln[f(y|\theta)])
    &=var\left\{ \frac{y-b'(\theta)}{a(\phi)}\right\}\\
    &= \frac{1}{\alpha(\phi)^2}var(y)
\end{align*}

so $var(y)=a(\phi)b''(\theta)$


\bigskip
{\bf Part D} \\
\bigskip

$E(y)= b'(\theta)=\mu$ and $var(y)=a(\phi)b''(\theta)=(1)(\sigma^2)=\sigma^2$\\

\bigskip

{\bf \large Generalized linear models} \\
\bigskip
\bigskip

{\bf Part A} \\
\bigskip

Show 

\begin{align*}
\theta_i &= (b')^{-1} \Big( g^{-1}(x_i^T \beta) \Big)  
\end{align*}
\begin{align*}
\mbox{var} (Y_i ) &=  \frac{\phi}{w_i} V(\mu_i)
\end{align*}

We know 

\begin{align*}
g(\mu_i) = x_i^T\beta\\
\mu_i = g^{-1}(x_i^T\beta) 
\end{align*}

And 

\begin{align*}
\mu_i = b'(\theta_i) \\ 
\end{align*}

So, 

\begin{align*}
  b'(\theta_i) = g^{-1}(x_i^T\beta)  \\ 
\end{align*}

Or 
\begin{align*}
  \theta_i= b'^{-1}[g^{-1}(x_i^T\beta)]  \\ 
\end{align*}


We also know

\begin{align*}
  \mbox{var}(Y_i)=a(\phi)g''(\theta_i)  \\ 
\end{align*}

By inspection, $a(\phi)=\phi/w_i$, and we know $\theta_i = b'^{-1}(\mu_i)$, and 

\begin{align*}
  b''(\theta_i)  = b''[b'^{-1}(\mu_i)]
\end{align*}

So $V(\mu_i)=b''[b'^{-1}(\mu_i)]$.\\

{\bf Part B} \\
\bigskip

(1) For the Poisson distribution, we know $\theta=log(\lambda)$ and $b(\theta)=\lambda=\exp(\theta)$.

\begin{align*}
    V(\mu_i)&=b''[b'^{-1}(\mu_i)]\\
    &=exp(\theta)[ln(\mu_i)]\\
    &= \mu_i
\end{align*}

(2) For the Binomial distribution, we know $\theta=\log[p/(1-p)]$ and $b(\theta)=\ln[1+\exp(\theta)]$. From this, we have that $b'(\theta)=\exp(\theta)/[1+\exp(\theta)]$ and $b''(\theta)=\exp(\theta)/[1+\exp(\theta)]^2$

\begin{align*}
    V(\mu_i)&=b''[b'^{-1}(\mu_i)]\\
    &=\exp\{\ln [\mu_i/(1-\mu_i)]\}/[1+\exp\{\ln [\mu_i/(1-\mu_i)]\} ]^2\\
    &= [\mu_i/(1-\mu_i)]/[1+\mu_i/(1-\mu_i)]^2\\
    &=\mu_i(1-\mu_i)
\end{align*}

{\bf Part C} \\
\bigskip
\bigskip

For the Poisson distribution we know that $b'^{-1}(\mu)=\log(\mu)$, so $g(\mu)=b'^{-1}(\mu) = \log(\mu)$\\

 
For the Binomial distribution we know that $b'^{-1}(\mu)=\log(-\mu/[\mu-1])$, so $g(\mu)=b'^{-1}(\mu) = \log(-\mu/[\mu-1])$

\bigskip
\bigskip


{\bf \large Fitting GLMs} \\
\bigskip
\bigskip

{\bf Part A
} \\
\bigskip
\bigskip

We know that $\mu_i = b'(\theta_i)$, so $\partial \mu_i / \partial \theta_i=b''(\theta_i) = b''(b'^{-1}(\mu_i))=V(\mu_i)$. \\

We also know that $x_i^T\beta = g(\mu_i)$, so $\beta = x_i^{-T}g(\mu_i)$ and $\partial \beta/ \partial \mu_i=x_i^{-T}g'(\mu_i)$

Using the chain rule, 

\begin{align*}
    s(\beta, \theta) &= \frac{\partial \log L}{\partial  \theta}  \frac{\partial \theta}{\partial \mu} \frac{\partial \mu}{\partial \beta}\\
    &= [ \frac{\partial}{\partial  \theta}   \sum_{i=1}^{n} \frac{y_i\theta_i-b(\theta_i)}{\phi/w_i} + c(y_i; \phi/w_i)]
    \frac{\partial \theta}{\partial \mu}  \frac{\partial \mu}{\partial \beta}\\
    &= [   \sum_{i=1}^{n} \frac{\partial}{\partial  \theta_i}  \frac{y_i\theta_i-b(\theta_i)}{\phi/w_i} + c(y_i; \phi/w_i)]
    \frac{\partial \theta_i}{\partial \mu_i}  \frac{\partial \mu_i}{\partial \beta}\\
    &=   \sum_{i=1}^{n} \frac{[y_i-b'(\theta_i)]x_i^T }{\phi/w_i V(\mu_i) g'(\mu_i)} 
\end{align*}

\bigskip

{\bf Part B
} \\
\bigskip
\bigskip

Under the canonical link, $g^{-1}=b'$. Then,

\begin{align*}
    V(\mu) &= b''[(b')^{-1}(\mu)]\\
    &= (g^{-1})'[g(\mu)]\\
     &= 1/g'(g^{-1}(g(\mu)))\\
     &=1/g'(\mu)
\end{align*}

So 

\begin{align*}
    s =  \sum_{i=1}^{n} \frac{[y_i-\mu_i]x_i }{\phi/w_i} 
\end{align*}

\bigskip

{\bf Part C
} \\
\bigskip
\bigskip

For the binomial distribution, we found that $b(\theta)=\log(1+\exp \theta)$. Under the canonical link $g=b'^{-1}$, so $g^{-1}(\theta)= \exp\theta/(1+\exp \theta)$ and $g(\theta)=\log(-\theta/[\theta-1])$. So,


\begin{align*}
    \mu_i &= b'(\theta)=p\\
\end{align*}

and

\begin{align*}
    g(\mu_i) &= x_i^T\beta \\
    \log(\mu_i/[1-\mu_i]) &= x_i^T\beta\\
    \mu_i/[1-\mu_i] &= \exp(x_i^T\beta)\\
    \mu_i &= \exp(x_i^T\beta)/[1+\exp(x_i^T\beta)])\\
    &= 1/[\exp(-x_i^T\beta)+1]
\end{align*}


So the score function is

\begin{align*}
    s =  \sum_{i=1}^{n} \frac{(y_i-1/[\exp(-x_i^T\beta)+1])x_i }{1/n} 
\end{align*}

And from earlier, the log likelihood is:\\


\begin{align*}
    \log L =  \sum_{i=1}^{n}  (y_in)\log[p_i] + n(1-y_i)\log[1-p_i]
\end{align*}

\bigskip

{\bf Part D
} \\
\bigskip
\bigskip


Find the Hessian of a GLM under the canonical link. We already know the score and need to differentiate it w.r.t $\beta_T$.

\begin{align*}
    s = \sum_{i=1}^{n} \frac{[y_i-\mu_i]x_i }{\phi/w_i} 
\end{align*}



\begin{align*}
    H &= \frac{\partial}{\partial \beta^T} \sum_{i=1}^{n} \frac{[y_i-\mu_i]x_i }{\phi/w_i} \\
    &= \frac{\partial}{\partial \beta^T} \sum_{i=1}^{n} \frac{[y_i-b'(x_i^T\beta)]x_i }{\phi/w_i} \\
    &=\sum_{i=1}^{n} \frac{-x_i^Tb''(\beta^T x_i)x_i }{\phi/w_i} \\
    &=-X^T\mbox{diag}[b''(X\beta)]X
\end{align*}


\bigskip

{\bf Part E
} \\
\bigskip
\bigskip
WRONG 
For the multivariate case, the second order Taylor expansion of the log likelihood is (let $q$ be the log likelihood)


\begin{align*}
   q(\beta) &= q(\beta_0) + s(\beta_0) (\beta-\beta_0) +\frac{1}{2}(\beta-\beta_0)^T H(\beta_0)(\beta-\beta_0)
\end{align*}

Given that $\beta_0$ is an approximation of $\beta$, assume that $s(\beta_0)\approx 0$, so

\begin{align*}
   q(\beta) &= q(\beta_0)  +\frac{1}{2}(\beta-\beta_0)^T H(\beta_0)(\beta-\beta_0)
\end{align*}

Now compare this to the desired form:

\begin{align*}
   q(\beta) &= -\frac{1}{2} (\Tilde{y}-X\beta)^T W(\Tilde{y}-X\beta) + c\\
   &= -\frac{1}{2} (X\beta_0-X\beta)^T W(X\beta_0-X\beta) + c\\
   &= -\frac{1}{2} (\beta_0-\beta)^TX^T WX(\beta_0-\beta) + c\\
   &= -\frac{1}{2} (\beta-\beta_0)^TX^T WX(\beta-\beta_0) + c\\
\end{align*}

We see that $c\equiv (\beta_0)$ and $X^TWX\equiv -H(\beta_0) $, or $W\equiv -X^{-T}H(\beta_0) X^{-1} \equiv \mbox{diag}[b''(X\beta)]$

Reminder, for logistic regression, $b''(x_i^T\beta) = e^{x_i^T\beta}/(1+ e^{x_i^T\beta})^2$

Find the tangent to $q$:

\begin{align*}
    \nabla q(\beta)=q'(\beta_0) - X^TWX(\beta-\beta_0)
\end{align*}

Set equal to 0 and solve for $\beta$

\begin{align*}
   0=q'(\beta) - 2H(\beta_0)(\beta-\beta_0)
\end{align*}

\begin{align*}
    H(\beta_0)^{-1}q'(\beta) = (\beta-\beta_0)
\end{align*}
This can be re-written as 
\begin{align*}
   \beta_{i+1}= \beta_{i}+H^{-1}\nabla q
\end{align*}


\begin{align*}
   H(\beta_{i+1}-\beta_{i})=\nabla q
\end{align*}

\end{document}